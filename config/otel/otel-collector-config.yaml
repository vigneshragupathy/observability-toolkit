receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus receiver for scraping metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

  # Kafka receiver (consumes produced log records for storage in Elasticsearch)
  kafka:
    brokers: [kafka:9092]
    topic: otel-logs
    group_id: otel-collector-log-consumer
    client_id: otel-collector
  # Use default OTLP protobuf encoding (receiver side); otlp_json not supported for receiver in this version

processors:
  # Batch processor for efficiency
  batch:
    timeout: 1s
    send_batch_size: 1024

  # Memory limiter to prevent OOM
  memory_limiter:
    limit_mib: 256
    check_interval: 1s

  # Resource processor to add metadata
  resource:
    attributes:
      - key: environment
        value: "dev"
        action: insert
      - key: collector.version
        value: "1.0.0"
        action: insert

  # Attributes processor for data enrichment
  attributes:
    actions:
      - key: http.user_agent
        action: delete
      - key: correlation.id
        from_attribute: trace_id
        action: insert

exporters:
  # OTLP exporter for traces to Jaeger
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true

  # Prometheus exporter for metrics
  prometheus:
    endpoint: "0.0.0.0:8889"
    const_labels:
      collector: "otel"

  # Elasticsearch exporter for logs
  elasticsearch:
    endpoints: ["http://elasticsearch:9200"]
    logs_index: "otel-logs"
    traces_index: "otel-traces"

  # Kafka exporter for logs (acts as a buffer/decoupling layer before ES)
  kafka/logs:
    brokers: [kafka:9092]
    topic: otel-logs
    # Use protobuf encoding for compatibility with receiver (otlp_json exporter encoding is experimental and not yet supported by receiver here)
    encoding: otlp_proto
    retry_on_failure:
      enabled: true
    metadata:
      full: true

  # Logging exporter for debugging
  logging:
    loglevel: debug

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  pprof:
    endpoint: 0.0.0.0:1777
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resource, attributes, batch]
      exporters: [otlp/jaeger, elasticsearch, logging]

    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, resource, batch]
      exporters: [prometheus, logging]

    # Stage 1: Receive logs from applications and produce to Kafka (no direct ES write)
    logs/produce:
      receivers: [otlp]
      processors: [memory_limiter, resource, attributes, batch]
      exporters: [kafka/logs, logging]

    # Stage 2: Consume logs from Kafka and store in Elasticsearch
    logs/consume:
      receivers: [kafka]
      processors: [batch]
      exporters: [elasticsearch, logging]

  telemetry:
    logs:
      level: "debug"
    metrics:
      address: 0.0.0.0:8888
